{
  "lstm_advanced": {
    "description": "Advanced LSTM with Attention and Residual Connections",
    "architecture": {
      "layers": [
        256,
        128,
        64,
        32
      ],
      "dropout": 0.3,
      "recurrent_dropout": 0.2,
      "bidirectional": true,
      "attention": true,
      "residual": true,
      "batch_norm": true,
      "optimizer": "AdamW",
      "learning_rate": 0.001,
      "epochs": 100,
      "early_stopping": true,
      "patience": 15
    },
    "expected_improvement": 15.0
  },
  "transformer_advanced": {
    "description": "Advanced Transformer with Multi-Head Attention",
    "architecture": {
      "heads": 12,
      "layers": 6,
      "d_model": 256,
      "dropout": 0.1,
      "positional_encoding": true,
      "layer_norm": true,
      "optimizer": "RAdam",
      "learning_rate": 0.0005,
      "epochs": 80,
      "early_stopping": true,
      "patience": 12
    },
    "expected_improvement": 20.0
  },
  "neural_network_advanced": {
    "description": "Deep Neural Network with Advanced Regularization",
    "architecture": {
      "layers": [
        512,
        256,
        128,
        64,
        32
      ],
      "dropout": 0.4,
      "batch_norm": true,
      "activation": "swish",
      "optimizer": "AdamW",
      "learning_rate": 0.001,
      "weight_decay": 0.01,
      "epochs": 120,
      "early_stopping": true,
      "patience": 20
    },
    "expected_improvement": 12.0
  }
}